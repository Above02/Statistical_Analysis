{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Above02/Statistical_Analysis/blob/master/Machine_Learning/code/Keras_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8FZgbmkvD5V"
   },
   "source": [
    "<h1>¿Keras y Tensorflow? ¿CNN?</h1>\n",
    "\n",
    "----\n",
    "\n",
    "Keras es una API de redes neuronales de alto nivel.\n",
    "\n",
    "Permite una experimentación rápida a través de una API de alto nivel, fácil de usar, modular y extensible. \n",
    "\n",
    "---\n",
    "Keras también se puede ejecutar tanto en CPU como en GPU.\n",
    "\n",
    "Platicaremos de los conceptos básicos de Keras, incluidos los dos modelos de Keras más utilizados (Secuencial y Funcional), las capas centrales y algunas funcionalidades de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCd4uCv5vkAc",
    "outputId": "3875ecd9-d279-4b28-e3c7-3828a86f5585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS6Km0BMvoYK"
   },
   "source": [
    "### Cargando en un conjunto de datos\n",
    "\n",
    "Keras proporciona siete conjuntos de datos diferentes, que se pueden cargar usando Keras directamente. \n",
    "\n",
    "Estos incluyen conjuntos de datos de imágenes, así como un precio de la vivienda y conjuntos de datos de reseñas de películas.\n",
    "\n",
    "En este pequeño ejercicio, usaremos el conjunto de datos MNIST, que contiene 70000 imágenes en escala de grises de 28x28 con 10 clases diferentes. \n",
    "\n",
    "---\n",
    "Keras lo divide en un conjunto de entrenamiento con 60000 instancias y un conjunto de pruebas con 10000 instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsAQmfxsvyWi",
    "outputId": "7aab75e5-c426-414e-9b98-27ff377679bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyOYUuw5v1hv"
   },
   "source": [
    "Para alimentar las imágenes a una red neuronal convolucional, transformamos el dataframe en cuatro dimensiones. Esto se puede hacer usando el método de reshape de Numpy. También transformaremos los datos en flotantes y los normalizaremos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pxeVzdtDv1oi"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mn_s4WYQwLmK"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoXIGEq5wO3X"
   },
   "source": [
    "### Creando un modelo con la API secuencial\n",
    "\n",
    "La forma más sencilla de crear un modelo en Keras es utilizando la API secuencial, que le permite apilar una capa tras otra. \n",
    "\n",
    "---\n",
    "El problema con la API secuencial es que no permite que los modelos tengan múltiples entradas o salidas, que son necesarias para algunos problemas.\n",
    "\n",
    "Sin embargo, la API secuencial es una elección perfecta para la mayoría de los problemas.\n",
    "\n",
    "Para crear una red neuronal convolucional solo necesitamos crear un objeto secuencial y usar la función agregar para agregar capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nqD0_4bZwW1N"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSJKPDUxwcPY"
   },
   "source": [
    "El código anterior primero crea un objeto secuencial y agrega algunas capas convolucionales, de agrupación máxima y de dropout (descarte). \n",
    "\n",
    "---\n",
    "Luego aplana la salida y le pasa una última capa densa y descarte antes de pasarla a nuestra capa de salida. \n",
    "\n",
    "### ¿ Qué es una red neuronal convolucional ?\n",
    "\n",
    "Las redes neuronales convolucionales son muy similares a las redes neuronales ordinarias: están formadas por neuronas que tienen pesos y bias que se pueden aprender. \n",
    "\n",
    "---\n",
    "Cada neurona recibe algunas entradas, realiza un producto escalar y, opcionalmente, lo sigue con una no linealidad. \n",
    "\n",
    "\n",
    "Toda la red todavía expresa una única función de puntuación diferenciable: desde los píxeles de la imagen sin procesar en un extremo hasta las puntuaciones de la clase en el otro. \n",
    "\n",
    "Y todavía tienen una función de pérdida (por ejemplo, SVM / Softmax) en la última capa (completamente conectada) y todavía se aplican todos los consejos / trucos que desarrollamos para aprender redes neuronales regulares.\n",
    "\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/19/41/9c/19419ca47404d8712f5ac4cd26b58c61.png\"/>\n",
    "\n",
    "---\n",
    "\n",
    "Entonces, ¿qué cambia? Las arquitecturas ConvNet asumen explícitamente que las entradas son imágenes, lo que nos permite codificar ciertas propiedades en la arquitectura. Estos hacen que la función de reenvío sea más eficiente de implementar y reducen enormemente la cantidad de parámetros en la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4s7mWL4yDG6"
   },
   "source": [
    "### Capas utilizadas para crear ConvNets\n",
    "\n",
    "Como describimos anteriormente, una ConvNet simple es una secuencia de capas, y cada capa de una ConvNet transforma un volumen de activaciones en otro a través de una función diferenciable. \n",
    "\n",
    "---\n",
    "Usamos tres tipos principales de capas para construir arquitecturas ConvNet: Capa convolucional, Capa de agrupación y Capa completamente conectada (exactamente como se ve en las redes neuronales normales). Apilaremos estas capas para formar una arquitectura ConvNet completa.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de Arquitectura: descripción general. \n",
    "\n",
    "Entraremos en más detalles a continuación, pero una ConvNet simple para la clasificación CIFAR-10 podría tener la arquitectura [INPUT - CONV - RELU - POOL - FC]. \n",
    "\n",
    "**Con más detalle:**\n",
    "\n",
    "* INPUT [32x32x3] mantendrá los valores de píxeles sin procesar de la imagen, en este caso una imagen de ancho 32, alto 32 y con tres canales de color R, G, B.\n",
    "* CONV layer calculará la salida de las neuronas que están conectadas a regiones locales en la entrada, cada una calculando un producto escalar entre sus pesos y una pequeña región a la que están conectadas en el volumen de entrada. Esto puede resultar en un volumen como [32x32x12] si decidimos usar 12 filtros.\n",
    "* RELU aplicará una función de activación por elementos, como max (0, x) y umbral en cero. Esto deja el tamaño del volumen sin cambios ([32x32x12]).\n",
    "* POOL realizará una operación de reducción de resolución a lo largo de las dimensiones espaciales (ancho, alto), dando como resultado un volumen como [16x16x12].\n",
    "* FC (es decir, completamente conectada) calculará los puntajes de la clase, lo que dará como resultado un volumen de tamaño [1x1x10], donde cada uno de los 10 números corresponde a un puntaje de clase, como entre las 10 categorías de CIFAR-10. Al igual que con las redes neuronales ordinarias y como su nombre lo indica, cada neurona de esta capa se conectará a todos los números del volumen anterior.\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/cnn/convnet.jpeg\"/>\n",
    "\n",
    "### En resumen:\n",
    "\n",
    "    Una arquitectura ConvNet es, en el caso más simple, una lista de capas que transforman el volumen de la imagen en un volumen de salida (por ejemplo, manteniendo las puntuaciones de la clase)\n",
    "    Hay algunos tipos distintos de capas (por ejemplo, CONV / FC / RELU / POOL son, con mucho, los más populares)\n",
    "    Cada capa acepta un volumen 3D de entrada y lo transforma en un volumen 3D de salida a través de una función diferenciable \n",
    "\n",
    "Nota: para mejor detalle, consultar el siguiente [link](https://cs231n.github.io/convolutional-networks/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1dgVf5kFvrn"
   },
   "source": [
    "<h1><center>¿Qúe es una CNN? ¿Cómo puede ver una red neuronal? ¿Cómo clasifica imagenes y distingue un perro de un gato?</center></h1>\n",
    "\n",
    "La CNN es un tipo de Red Neuronal Artificial con aprendizaje supervisado que procesa sus capas imitando al cortex visual del ojo humano para identificar distintas características en las entradas que en definitiva hacen que pueda identificar objetos y “ver”. \n",
    "\n",
    "Para ello, la CNN contiene varias capas ocultas especializadas y con una jerarquía: esto quiere decir que las primeras capas pueden detectar lineas, curvas y se van especializando hasta llegar a capas más profundas que reconocen formas complejas como un rostro o la silueta de un animal.\n",
    "\n",
    "---\n",
    "\n",
    "Recodemos que la red neuronal deberá aprender por sí sola a reconocer una diversidad de objetos dentro de imágenes y para ello necesitaremos una gran cantidad de imágenes -lease más de 10.000 imágenes de gatos, otras 10.000 de perros,…- para que la red pueda captar sus características únicas -de cada objeto- y a su vez, poder generalizarlo -esto es que pueda reconocer como gato tanto a un felino negro, uno blanco, un gato de frente, un gato de perfil, gato saltando, etc.-\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-01.png?resize=768%2C434&ssl=1\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### Pixeles y neuronas\n",
    "\n",
    "Para comenzar, la red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a  784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada. Para continuar con el ejemplo, supondremos que utilizamos la imagen con 1 sólo color.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-02.png?resize=768%2C567&ssl=1\"/>\n",
    "\n",
    "### Pre-procesamiento\n",
    "\n",
    "Antes de alimentar la red, recuerda que como entrada nos conviene normalizar los valores. Los colores de los pixeles tienen valores que van de 0 a 255, haremos una transformación de cada pixel: “valor/255” y nos quedará siempre un valor entre 0 y 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Convoluciones\n",
    "\n",
    "Ahora comienza el “procesado distintivo” de las CNN. \n",
    "\n",
    "Es decir, haremos las llamadas “convoluciones”: Estas consisten en tomar “grupos de pixeles cercanos” de la imagen de entrada e ir operando matemáticamente (producto escalar) contra una pequeña matriz que se llama kernel. \n",
    "\n",
    "<img src=\"https://i1.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-03.png?resize=300%2C219&ssl=1\"/>\n",
    "\n",
    "Ese kernel supongamos de tamaño 3×3 pixels “recorre” todas las neuronas de entrada (de izquierda-derecha, de arriba-abajo) y genera una nueva matriz de salida, que en definitiva será nuestra nueva capa de neuronas ocultas. \n",
    "\n",
    "* NOTA: si la imagen fuera a color, el kernel realmente sería de 3x3x3 es decir: un filtro con 3 kernels de 3×3; luego  esos 3 filtros se suman (y se le suma una unidad bias) y conformarán 1 salida (cómo si fuera 1 solo canal).\n",
    "\n",
    "--------\n",
    "\n",
    "### Filtro: conjunto de kernels\n",
    "\n",
    "<img src=\"https://i2.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn_kernel.gif?resize=406%2C134\"/>\n",
    "\n",
    "A medida que vamos desplazando el kernel y vamos obteniendo una “nueva imagen” filtrada por el kernel. En esta primer convolución y siguiendo con el ejemplo anterior, es como si obtuviéramos 32 “imágenes filtradas nuevas”. Estas imágenes nuevas lo que están “dibujando” son ciertas características de la imagen original. Esto ayudará en el futuro a poder distinguir un objeto de otro (por ej. gato ó perro).\n",
    "\n",
    "<img src=\"https://i2.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/CNN-04.png?resize=768%2C355&ssl=1\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### La función de Activación\n",
    "\n",
    "La más utilizada para este tipo de redes neuronales es la llamada ReLu por Rectifier Linear Unit  y consiste en $f(x)=max(0,x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "Ahora viene un paso en el que reduciremos la cantidad de neuronas antes de hacer una nueva convolución. \n",
    "\n",
    "**¿Por qué?** \n",
    "\n",
    "Como vimos, a partir de nuestra imagen blanco y negro de 28x28px tenemos una primer capa de entrada de 784 neuronas y luego de la primer convolución obtenemos una capa oculta de 25.088 neuronas -que realmente son nuestros 32 mapas de características de 28×28-\n",
    "\n",
    "Si hiciéramos una nueva convolución a partir de esta capa, el número de neuronas de la próxima capa se iría por las nubes (y ello implica mayor procesamiento)! \n",
    "\n",
    "---\n",
    "Para reducir el tamaño de la próxima capa de neuronas haremos un proceso de **subsampling** en el que reduciremos el tamaño de nuestras imágenes filtradas pero en donde deberán prevalecer las características más importantes que detectó cada filtro.\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-05.png?resize=300%2C233&ssl=1\"/>\n",
    "\n",
    "### Subsampling con Max-Pooling\n",
    "\n",
    "Vamos a intentar explicarlo con un ejemplo: supongamos que haremos Max-pooling de tamaño 2×2. Esto quiere decir que recorreremos cada una de nuestras 32 imágenes de características obtenidas anteriormente de 28x28px de izquierda-derecha, arriba-abajo.... \n",
    "\n",
    "PERO en vez de tomar de a 1 pixel, tomaremos de “2×2” (2 de alto por 2 de ancho = 4 pixeles) e iremos preservando el valor “más alto” de entre esos 4 pixeles (por eso lo de “Max”). En este caso, usando 2×2, la imagen resultante es reducida “a la mitad”y quedará de 14×14 pixeles. Luego de este proceso de subsamplig nos quedarán  32 imágenes de 14×14, pasando de haber tenido 25.088 neuronas a  6272, son bastantes menos y -en teoría- siguen almacenando la información más importante para detectar características deseadas.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Primera convolución\n",
    "\n",
    "<img src=\"https://i2.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-06.png?resize=768%2C576&ssl=1\"/>\n",
    "\n",
    "Primer convolución: consiste de una entrada, un conjunto de filtros, generamos un mapa de características y hacemos un subsampling. Con lo cual, en el ejemplo de imágenes de 1 sólo color tendremos lo de la imagen anterior... y sería\n",
    "\n",
    "* Entrada: 28x28x1\n",
    "* Kernel: 32 filtros de 3x3\n",
    "* Feature Mapping: 28x28x32\n",
    "* Max-Pooling: 2x2\n",
    "* Salida: 14x14x32\n",
    "\n",
    "---\n",
    "\n",
    "La primer convolución es capaz de detectar características primitivas como lineas ó curvas. A medida que hagamos más capas con las convoluciones, los mapas de características serán capaces de reconocer formas más complejas, y el conjunto total de capas de convoluciones podrá “ver”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_cyEg8RH1YO"
   },
   "source": [
    "#### Pues ahora deberemos hacer una Segunda convolución que será:\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/cnn-07.png?resize=768%2C576&ssl=1\"/>\n",
    "\n",
    "* Entrada: 14x14x32\n",
    "* Kernel: 64 filtros de 3x3\n",
    "* Feature Mapping: 14x14x64\n",
    "* Max-Pooling: 2x2\n",
    "* Salida: 7x7x64\n",
    "\n",
    "---\n",
    "\n",
    "La 3er convolución comenzará en tamaño 7×7 pixels y luego del max-pooling quedará en 3×3 con lo cual podríamos hacer sólo 1 convolución más. En este ejemplo empezamos con una imagen de 28x28px e hicimos 3 convoluciones. Si la imagen inicial hubiese sido mayor (de 224x224px) aún hubiéramos podido seguir haciendo convoluciones.\n",
    "\n",
    "* Entrada: 7x7x32\n",
    "* Kernel: 128 filtros de 3x3\n",
    "* Feature Mapping: 7x7x64\n",
    "* Max-Pooling: 2x2\n",
    "* Salida: 3x3x128\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Conectar con una red neuronal “tradicional”\n",
    "\n",
    "Para terminar, tomaremos la última capa oculta a la que hicimos subsampling, que se dice que es “tridimensional” por tomar la forma -en nuestro ejemplo- 3x3x128 (alto,ancho,mapas) y la “aplanamos”, esto es que deja de ser tridimensional, y pasa a ser una capa de neuronas “tradicionales”, de las que ya conocíamos. Por ejemplo, podríamos aplanar (y conectar) a una nueva capa oculta de 100 neuronas feedforward.\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2018/11/CNN-08.png?resize=768%2C459&ssl=1\"/>\n",
    "\n",
    "Entonces, a esta nueva capa oculta “tradicional”, le aplicamos una función llamada **Softmax** que conecta contra la capa de salida final que tendrá la cantidad de neuronas correspondientes con las clases que estamos clasificando.\n",
    "\n",
    "Si clasificamos perros y gatos, serán 2 neuronas. Si es el dataset **Mnist** serán 10 neuronas de salida. Si clasificamos coches, aviones ó barcos serán 3, etc.\n",
    "\n",
    "Las salidas al momento del entrenamiento tendrán el formato conocido como “one-hot-encoding” en el que para perros y gatos sera: [1,0] y [0,1], para coches, aviones ó barcos será [1,0,0]; [0,1,0];[0,0,1].\n",
    "\n",
    "Y la función de Softmax se encarga de pasar a probabilidad (entre 0 y 1) a las neuronas de salida. Por ejemplo una salida [0,2 0,8] nos indica 20% probabilidades de que sea perro y 80% de que sea gato.\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### ¿Y cómo aprendió la CNN a “ver”?: Backpropagation\n",
    "\n",
    "El proceso es similar al de las redes tradicionales en las que tenemos una entrada y una salida esperada (por eso **aprendizaje supervisado**) y mediante el backpropagation mejoramos el valor de los pesos de las interconexiones entre capas de neuronas y a medida que iteramos esos pesos se ajustan hasta ser óptimos. **PERO…**\n",
    "\n",
    "En el caso de las CNN, deberemos ajustar el valor de los pesos de los distintos kernels. Esto es una gran ventaja al momento del aprendizaje pues como vimos cada kernel es de un tamaño reducido, en nuestro ejemplo en la primer convolución es de tamaño de 3×3, eso son sólo 9 parámetros que debemos ajustar en 32 filtros dan un total de 288 parámetros. \n",
    "\n",
    "En comparación con los pesos entre dos capas de neuronas “tradicionales”: una de 748 y otra de  6272 en donde están TODAS interconectarlas con TODAS y eso equivaldría a tener que entrenar y ajustar más de 4,5 millones de pesos (repito: sólo para 1 capa).\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "### Arquitectura básica\n",
    "\n",
    "Resumiendo: podemos decir que los elementos que usamos para crear CNNs son:\n",
    "\n",
    "* Entrada: Serán los pixeles de la imagen. Serán alto, ancho y profundidad será 1 sólo color o 3 para Red,Green,Blue.\n",
    "* Capa De Convolución: procesará la salida de neuronas que están conectadas en “regiones locales” de entrada (es decir pixeles cercanos), calculando el producto escalar entre sus pesos (valor de pixel) y una pequeña región a la que están conectados en el volumen de entrada. Aquí usaremos por ejemplo 32 filtros o la cantidad que decidamos y ese será el volumen de salida.\n",
    "* “CAPA RELU” aplicará la función de activación en los elementos de la matriz.\n",
    "* POOL ó SUBSAMPLING: Hará una reducción en las dimensiones alto y ancho, pero se mantiene la profundidad.\n",
    "* CAPA “TRADICIONAL” red de neuronas feedforward que conectará con la última capa de subsampling y finalizará con la cantidad de neuronas que queremos clasificar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ4P_cTszycd"
   },
   "source": [
    "Regresando con Keras, también se puede construir el modelo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wFNouXsTzp1H"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]),\n",
    "    Conv2D(filters=32, kernel_size=(5,5), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Dropout(rate=0.25),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Dropout(rate=0.25),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMZmMTwwz6-A"
   },
   "source": [
    "### Creando un modelo con la API\n",
    "\n",
    "Alternativamente, la API le permite crear los mismos modelos pero le ofrece más flexibilidad a costa de simplicidad y legibilidad.\n",
    "\n",
    "Se puede utilizar con múltiples capas de entrada y salida, así como con capas compartidas, lo que le permite construir estructuras de red realmente complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t_Z-DDk6z6Lz"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Input\n",
    "\n",
    "inputs = Input(shape=x_train.shape[1:])\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(inputs)\n",
    "x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmBEXKzE0Gno"
   },
   "source": [
    "### Compilar un modelo\n",
    "\n",
    "Antes de que podamos comenzar a entrenar nuestro modelo, debemos configurar el proceso de aprendizaje. Para esto, necesitamos especificar un optimizador, una función de pérdida y opcionalmente algunas métricas como la precisión.\n",
    "\n",
    "La función de pérdida es una medida de qué tan bueno es nuestro modelo para lograr el objetivo dado.\n",
    "\n",
    "Se usa un optimizador para minimizar la función de pérdida (objetivo) actualizando los pesos usando los gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5f1TOR1e0Gu_"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQnnD-Id0QMZ"
   },
   "source": [
    "### Aumento de datos de imagen\n",
    "\n",
    "El aumento es un proceso de creación de más datos a partir de existentes. Para las imágenes, puede realizar pequeñas transformaciones como rotar la imagen, hacer zoom en la imagen, agregar ruido y muchas más.\n",
    "\n",
    "Esto ayuda a que el modelo sea más robusto y resuelve el problema de no tener suficientes datos. Keras tiene un método llamado ImageDataGenerator que puede usarse para aumentar imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qRayZ4Y40QSc"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "  rotation_range=10,\n",
    "  zoom_range=0.1,\n",
    "  width_shift_range=0.1,\n",
    "  height_shift_range=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL74dcxf0eeF"
   },
   "source": [
    "Este ImageDataGenerator creará nuevas imágenes que se rotaron, acercaron o alejaron y se cambiaron de ancho y alto.\n",
    "\n",
    "---\n",
    "\n",
    "### Ajustar un modelo\n",
    "\n",
    "Ahora que definimos y compilamos nuestro modelo, estamos listos para el entrenamiento. \n",
    "\n",
    "---\n",
    "Para entrenar un modelo, normalmente usaríamos el método de ajuste, pero debido a que estamos usando un generador de datos, usaremos fit_generator y le pasaremos nuestro generador, datos X, datos y, así como el número de épocas y el tamaño del lote.\n",
    "\n",
    "---\n",
    "También le pasaremos un conjunto de validación para que podamos monitorear la pérdida y la precisión en ambos conjuntos, así como también en steps_per_epoch, que se requiere cuando se usa un generador y simplemente se establece en la longitud del conjunto de entrenamiento dividido por batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEzI6PJn00db",
    "outputId": "5b9bbafd-2043-462b-8350-63e8cc426069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ad0ba772c870>:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.3287 - accuracy: 0.8964 - val_loss: 0.0373 - val_accuracy: 0.9878\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1142 - accuracy: 0.9670 - val_loss: 0.0267 - val_accuracy: 0.9903\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0882 - accuracy: 0.9751 - val_loss: 0.0296 - val_accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs,\n",
    "                              validation_data=(x_test, y_test), steps_per_epoch=x_train.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVM26Tht03fR"
   },
   "source": [
    "### Visualizando el proceso de entrenamiento\n",
    "\n",
    "Podemos visualizar nuestra precisión y pérdida de entrenamiento y prueba para cada época para que podamos tener una intuición sobre el rendimiento de nuestro modelo. \n",
    "\n",
    "---\n",
    "La precisión y la pérdida a lo largo de las épocas se guardan en la variable de historial que obtuvimos durante el entrenamiento y usaremos Matplotlib para visualizar estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "CvZuAaid0ejM",
    "outputId": "d1e4df31-1d9a-4a53-e1a1-590d4f362987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f40603eb5c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='testing accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "V2z8TRUL1AK5",
    "outputId": "77c4d1e9-2b63-4e26-dea4-66c8b7d1c822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f40600fd080>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='testing loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWs4BThThslH"
   },
   "source": [
    "### Transferir el aprendizaje en Keras con modelos de visión por computadora\n",
    "\n",
    "---\n",
    "\n",
    "Los modelos de redes neuronales profundas pueden tardar días o incluso semanas en entrenarse en conjuntos de datos muy grandes.\n",
    "\n",
    "Una forma de acortar este proceso es reutilizar los pesos del modelo de modelos previamente entrenados que se desarrollaron para conjuntos de datos de referencia de visión por computadora estándar, como las tareas de reconocimiento de imágenes de ImageNet. Los modelos de alto rendimiento se pueden descargar y utilizar directamente, o integrar en un nuevo modelo para sus propios problemas de visión por computadora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoRM5n7Jh6WR"
   },
   "source": [
    "### ¿Qué es el aprendizaje por transferencia?\n",
    "\n",
    "El aprendizaje por transferencia generalmente se refiere a un proceso en el que un modelo entrenado en un problema se usa de alguna manera en un segundo problema relacionado.\n",
    "\n",
    "En el aprendizaje profundo, el aprendizaje por transferencia es una técnica mediante la cual un modelo de red neuronal se entrena primero en un problema similar al problema que se está resolviendo. Luego, una o más capas del modelo entrenado se utilizan en un nuevo modelo entrenado en el problema de interés.\n",
    "\n",
    "---\n",
    "\n",
    "El aprendizaje de transferencia tiene la ventaja de disminuir el tiempo de entrenamiento para un modelo de red neuronal y puede resultar en un menor error de generalización.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Cómo utilizar modelos previamente entrenados\n",
    "\n",
    "El uso de un modelo previamente entrenado está limitado solo por tu creatividad....\n",
    "\n",
    "Por ejemplo, un modelo puede descargarse y usarse tal cual, como incrustado en una aplicación y usado para clasificar nuevas fotografías.\n",
    "\n",
    "Alternativamente, los modelos se pueden descargar y usar como modelos de extracción de características. Aquí, la salida del modelo de una capa anterior a la capa de salida del modelo se utiliza como entrada para un nuevo modelo clasificador.\n",
    "\n",
    "Recuerda que las capas convolucionales más cercanas a la capa de entrada del modelo aprenden características de bajo nivel como líneas, que las capas en el medio de la capa aprenden características abstractas complejas que combinan las características de nivel inferior extraídas de la entrada y capas más cercanas a la salida. interpretar las características extraídas en el contexto de una tarea de clasificación.\n",
    "\n",
    "Con esta comprensión, se puede elegir un nivel de detalle para la extracción de características de un modelo existente previamente entrenado. \n",
    "\n",
    "Por ejemplo, si una nueva tarea es bastante diferente de clasificar objetos en fotografías (por ejemplo, diferente a ImageNet), entonces quizás la salida del modelo previamente entrenado después de las pocas capas sería apropiada. Si una nueva tarea es bastante similar a la tarea de clasificar objetos en fotografías, entonces quizás se pueda usar la salida de capas mucho más profundas en el modelo, o incluso la salida de la capa completamente conectada antes de la capa de salida.\n",
    "\n",
    "----\n",
    "\n",
    "El modelo previamente entrenado se puede utilizar como un programa de extracción de características independiente, en cuyo caso la entrada puede ser procesada previamente por el modelo o parte del modelo a una salida dada (por ejemplo, vector de números) para cada imagen de entrada, que puede luego utilícelo como entrada al entrenar un nuevo modelo.\n",
    "\n",
    "Alternativamente, el modelo previamente entrenado o la parte deseada del modelo se puede integrar directamente en un nuevo modelo de red neuronal. En este caso, los pesos de los preentrenados se pueden congelar para que no se actualicen a medida que se entrena el nuevo modelo. Alternativamente, los pesos pueden actualizarse durante el entrenamiento del nuevo modelo, quizás con una tasa de aprendizaje más baja, lo que permite que el modelo preentrenado actúe como un esquema de inicialización de pesos al entrenar el nuevo modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Modelos para la transferencia de aprendizaje\n",
    "\n",
    "Quizás haya una docena o más de [modelos](https://keras.io/api/applications/) de alto rendimiento para el reconocimiento de imágenes que se pueden descargar y utilizar como base para el reconocimiento de imágenes y las tareas relacionadas con la visión por computadora.\n",
    "\n",
    "Quizás tres de los modelos más populares son los siguientes:\n",
    "\n",
    "    VGG (por ejemplo, VGG16 o VGG19).\n",
    "    GoogLeNet (por ejemplo, InceptionV3).\n",
    "    Red residual (por ejemplo, ResNet50).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbLEmE3xhrs7",
    "outputId": "179c9a0f-e650-4a68-e55f-e8e00ed2e6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 0us/step\n",
      "Doberman (33.59%)\n"
     ]
    }
   ],
   "source": [
    "# example of using a pre-trained model as a classifier\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "image = load_img('/content/drive/MyDrive/Colab Notebooks/Datasets/dog.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load the model\n",
    "model = VGG16()\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekv1NZtZjW5T"
   },
   "source": [
    "### Modelo previamente entrenado como preprocesador de extracción de características\n",
    "\n",
    "El modelo previamente entrenado se puede utilizar como un programa independiente para extraer características de nuevas fotografías.\n",
    "\n",
    "Específicamente, las características extraídas de una fotografía pueden ser un vector de números que el modelo utilizará para describir las características específicas de una fotografía. Luego, estas características se pueden utilizar como entrada en el desarrollo de un nuevo modelo.\n",
    "\n",
    "Las últimas capas del modelo VGG16 son capas completamente conectadas antes de la capa de salida. Estas capas proporcionarán un conjunto complejo de características para describir una imagen de entrada determinada y pueden proporcionar información útil al entrenar un nuevo modelo para la clasificación de imágenes o una tarea relacionada con la visión por computadora.\n",
    "\n",
    "La imagen se puede cargar y preparar para el modelo, como hicimos antes en el ejemplo anterior.\n",
    "\n",
    "Cargaremos el modelo con la parte de salida del clasificador del modelo, pero eliminaremos manualmente la capa de salida final. Esto significa que la penúltima capa completamente conectada con 4096 nodos será la nueva capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pA0_gopjXBM",
    "outputId": "c8266715-ab7c-419a-ed9a-dd46466dceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "# example of using the vgg16 model as a feature extraction model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from pickle import dump\n",
    "# load an image from file\n",
    "image = load_img('/content/drive/MyDrive/Colab Notebooks/Datasets/dog.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "# get extracted features\n",
    "features = model.predict(image)\n",
    "print(features.shape)\n",
    "# save to file\n",
    "dump(features, open('dog.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smZuNA2-joOR"
   },
   "source": [
    "Al ejecutar el ejemplo, se carga la fotografía y luego se prepara el modelo como modelo de extracción de características.\n",
    "\n",
    "Las características se extraen de la foto cargada y se imprime la forma del vector de características, lo que muestra que tiene 4096 números. Esta característica luego se guarda en un nuevo archivo dog.pkl en el directorio de trabajo actual.\n",
    "\n",
    "---\n",
    "\n",
    "### Modelo previamente entrenado como extractor de características en el modelo\n",
    "\n",
    "Podemos usar algunas o todas las capas en un modelo previamente entrenado como un componente de extracción de características de un nuevo modelo directamente.\n",
    "\n",
    "Esto se puede lograr cargando el modelo y luego simplemente agregando nuevas capas. Esto puede implicar agregar nuevas capas convolucionales y agrupadas para expandir las capacidades de extracción de características del modelo o agregar nuevas capas de tipo de clasificador completamente conectadas para aprender cómo interpretar las características extraídas en un nuevo conjunto de datos o alguna combinación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vn6yb53j50x",
    "outputId": "a8758415-d978-4530-ff21-2832bb5fe192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 300, 300, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 300, 300, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 150, 150, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 150, 150, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 75, 75, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              42468352  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 57,193,290\n",
      "Trainable params: 57,193,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of tending the vgg16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "# load model without classifier layers\n",
    "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(1024, activation='relu')(flat1)\n",
    "output = Dense(10, activation='softmax')(class1)\n",
    "# define new model\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "# summarize\n",
    "model.summary()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu2gAFegkHMW"
   },
   "source": [
    "Un buen ejempllo de cómo mejorar el desempeño lo podemos leer [aquí](https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras_CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
